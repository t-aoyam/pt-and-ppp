{
"lmtrainer":
	{
	"data_size_for_lm": "1_000_000_000",
	"data_size_for_tokenizer": "5_000_000",
	"target_num_toks_for_lm": "10_000_000_000",
	"context_length": "1_024",
	"save_at_n_words": ["500_000", "1_000_000", "2_000_000", "4_000_000", "8_000_000", "16_000_000", "32_000_000", "64_000_000", "128_000_000", "256_000_000", "500_000_000", "1_000_000_000", "1_500_000_000", "2_000_000_000", "2_500_000_000", "3_000_000_000", "3_500_000_000", "4_000_000_000", "4_500_000_000", "5_000_000_000", "5_500_000_000", "6_000_000_000", "6_500_000_000", "7_000_000_000", "7_500_000_000", "8_000_000_000", "8_500_000_000", "9_000_000_000", "9_500_000_000", "10_000_000_000"],
	"seed": "1",
	"layers_to_unfreeze": []
	},
"lm":
	{
	"n_embd": "768",
	"n_head": "8"
	},
"lm_training":
	{
	"learning_rate": "2.5e-4",
	"lr_scheduler_type": "cosine",
	"warmup_steps": "2_000",
	"per_device_train_batch_size": "4",
	"gradient_accumulation_steps": "1"
	}
}
